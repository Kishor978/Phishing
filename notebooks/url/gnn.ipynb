{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7422661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a41a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Tokenization and Vocabulary --------------------\n",
    "def tokenize_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    tokens = []\n",
    "    if parsed.scheme:\n",
    "        tokens.append(parsed.scheme)\n",
    "    if parsed.hostname:\n",
    "        tokens += parsed.hostname.split('.')\n",
    "    if parsed.path:\n",
    "        tokens += parsed.path.strip(\"/\").split(\"/\")\n",
    "    if parsed.query:\n",
    "        tokens += list(parse_qs(parsed.query).keys())\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(df, max_vocab_size=5000):\n",
    "    token_counts = Counter()\n",
    "    for url in df['text']:\n",
    "        tokens = tokenize_url(url)\n",
    "        token_counts.update(tokens)\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for i, (token, _) in enumerate(token_counts.most_common(max_vocab_size), start=2):\n",
    "        vocab[token] = i\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15694b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- URL to Graph Conversion --------------------\n",
    "def url_to_graph(url, label, vocab, max_nodes=30):\n",
    "    tokens = tokenize_url(url)[:max_nodes]\n",
    "    node_ids = [vocab.get(tok, vocab['<UNK>']) for tok in tokens]\n",
    "    num_nodes = len(node_ids)\n",
    "    if num_nodes < 2:\n",
    "        return None\n",
    "    edge_index = torch.tensor([[i, i + 1] for i in range(num_nodes - 1)], dtype=torch.long).t()\n",
    "    x = torch.tensor(node_ids, dtype=torch.long).unsqueeze(1)\n",
    "    return Data(x=x, edge_index=edge_index, y=torch.tensor([label], dtype=torch.float))\n",
    "# -------------------- PyG Dataset --------------------\n",
    "class URLGraphDataset(InMemoryDataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.vocab = vocab\n",
    "        super().__init__('.')\n",
    "        data_list = [url_to_graph(url, label, vocab) for url, label in zip(df['text'], df['label']) if url_to_graph(url, label, vocab)]\n",
    "        self.data, self.slices = self.collate(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59239162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- GCN Model --------------------\n",
    "class URLGNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv1 = GCNConv(embed_dim, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.embedding(data.x.squeeze(1))\n",
    "        x = torch.relu(self.conv1(x, data.edge_index))\n",
    "        x = torch.relu(self.conv2(x, data.edge_index))\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        x = self.dropout(x)\n",
    "        return torch.sigmoid(self.fc(x)).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0baaae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Train Function with TQDM + EarlyStopping --------------------\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler=None, epochs=30, patience=5):\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_preds': [], 'val_probs': [], 'val_labels': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "            preds = (out > 0.5).int()\n",
    "            correct += (preds == batch.y.int()).sum().item()\n",
    "            total += batch.num_graphs\n",
    "        train_loss = total_loss / total\n",
    "        train_acc = correct / total\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_preds, val_probs, val_labels = 0, [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch)\n",
    "                loss = criterion(out, batch.y)\n",
    "                val_loss += loss.item() * batch.num_graphs\n",
    "                probs = out.cpu().numpy().tolist()\n",
    "                preds = (out > 0.5).int().cpu().numpy().tolist()\n",
    "                labels = batch.y.int().cpu().numpy().tolist()\n",
    "                val_probs += probs\n",
    "                val_preds += preds\n",
    "                val_labels += labels\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds)\n",
    "\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_preds'] = val_preds\n",
    "        history['val_probs'] = val_probs\n",
    "        history['val_labels'] = val_labels\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}: Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | Val Loss={val_loss:.4f}, Acc={val_acc:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_f1)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_model_wts = model.state_dict()\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_gnn_model.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"\\nâ›” Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0649d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Plotting Functions --------------------\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Legit', 'Phish'], yticklabels=['Legit', 'Phish'])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_probs):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b15da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# -------------------- Main Pipeline --------------------\n",
    " # columns: URL, label\n",
    "df = pd.read_json(r\"E:\\Phising_detection\\dataset\\urls\\urls.json\")\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
    "vocab = build_vocab(train_df)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "train_data = URLGraphDataset(train_df, vocab)\n",
    "val_data = URLGraphDataset(val_df, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = URLGNN(vocab_size=len(vocab)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model, history = train_model(model, train_loader, val_loader, optimizer, criterion, scheduler=scheduler, epochs=30, patience=5)\n",
    "\n",
    "plot_metrics(history)\n",
    "plot_confusion_matrix(history['val_labels'], history['val_preds'])\n",
    "plot_roc_curve(history['val_labels'], history['val_probs'])\n",
    "\n",
    "torch.save(model.state_dict(), \"best_gnn_model.pt\")\n",
    "with open(\"vocab.json\", \"w\") as f:\n",
    "    json.dump(vocab, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Phising_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
