{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c02939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Tokenizer and Vocab Builder --------------------\n",
    "def build_char_vocab(df, max_len=200):\n",
    "    chars = Counter()\n",
    "    for url in df['text']:\n",
    "        chars.update(url[:max_len])\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for i, (ch, _) in enumerate(chars.most_common(), start=2):\n",
    "        vocab[ch] = i\n",
    "    return vocab\n",
    "\n",
    "def tokenize_char_url(url, vocab, max_len=200):\n",
    "    tokens = [vocab.get(c, vocab['<UNK>']) for c in url[:max_len]]\n",
    "    if len(tokens) < max_len:\n",
    "        tokens += [vocab['<PAD>']] * (max_len - len(tokens))\n",
    "    return torch.tensor(tokens)\n",
    "\n",
    "def tokenize_graph_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    tokens = []\n",
    "    if parsed.scheme:\n",
    "        tokens.append(parsed.scheme)\n",
    "    if parsed.hostname:\n",
    "        tokens += parsed.hostname.split('.')\n",
    "    if parsed.path:\n",
    "        tokens += parsed.path.strip(\"/\").split(\"/\")\n",
    "    return tokens[:30]\n",
    "\n",
    "def build_graph_vocab(df):\n",
    "    token_counter = Counter()\n",
    "    for url in df['text']:\n",
    "        token_counter.update(tokenize_graph_url(url))\n",
    "    vocab = {'<UNK>': 0}\n",
    "    for i, (tok, _) in enumerate(token_counter.items(), start=1):\n",
    "        vocab[tok] = i\n",
    "    return vocab\n",
    "\n",
    "def url_to_graph(url, label, vocab):\n",
    "    tokens = tokenize_graph_url(url)\n",
    "    node_ids = [vocab.get(t, vocab['<UNK>']) for t in tokens]\n",
    "    if len(node_ids) < 2:\n",
    "        return None\n",
    "    edge_index = torch.tensor([[i, i+1] for i in range(len(node_ids)-1)], dtype=torch.long).t()\n",
    "    x = torch.tensor(node_ids, dtype=torch.long).unsqueeze(1)\n",
    "    return Data(x=x, edge_index=edge_index, y=torch.tensor([label], dtype=torch.float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c959ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Fusion Dataset --------------------\n",
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, df, char_vocab, graph_vocab, max_len=200):\n",
    "        self.data = []\n",
    "        for _, row in df.iterrows():\n",
    "            char_tensor = tokenize_char_url(row['text'], char_vocab, max_len)\n",
    "            graph_data = url_to_graph(row['text'], row['label'], graph_vocab)\n",
    "            if graph_data is not None:\n",
    "                self.data.append((char_tensor, graph_data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e319843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- CharCNN Module --------------------\n",
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32, seq_len=200):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(128, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(0, 2, 1)  # [B, E, L]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x).squeeze(-1)  # [B, 128]\n",
    "        x = self.fc(x)\n",
    "        return x  # [B, 64]\n",
    "\n",
    "# -------------------- GNN Module --------------------\n",
    "class URLGNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv1 = GCNConv(embed_dim, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.embedding(data.x.squeeze(1))\n",
    "        x = F.relu(self.conv1(x, data.edge_index))\n",
    "        x = F.relu(self.conv2(x, data.edge_index))\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        return x  # [B, 64]\n",
    "\n",
    "# -------------------- Fusion Model --------------------\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, cnn_vocab_size, gnn_vocab_size):\n",
    "        super().__init__()\n",
    "        self.cnn = CharCNN(cnn_vocab_size)\n",
    "        self.gnn = URLGNN(gnn_vocab_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, char_input, graph_data):\n",
    "        x1 = self.cnn(char_input)\n",
    "        x2 = self.gnn(graph_data)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return torch.sigmoid(self.fc2(x)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848449c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fusion(model, train_loader, val_loader, optimizer, criterion, scheduler=None, epochs=30, patience=5):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_preds': [], 'val_probs': [], 'val_labels': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        for char_input, graph_data in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "            char_input = char_input.to(device)\n",
    "            graph_data = graph_data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(char_input, graph_data)\n",
    "            loss = criterion(out, graph_data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * graph_data.num_graphs\n",
    "            preds = (out > 0.5).int()\n",
    "            correct += (preds == graph_data.y.int()).sum().item()\n",
    "            total += graph_data.num_graphs\n",
    "        train_acc = correct / total\n",
    "        history['train_loss'].append(train_loss / total)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_preds, val_probs, val_labels = 0, [], [], []\n",
    "        with torch.no_grad():\n",
    "            for char_input, graph_data in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "                char_input = char_input.to(device)\n",
    "                graph_data = graph_data.to(device)\n",
    "                out = model(char_input, graph_data)\n",
    "                loss = criterion(out, graph_data.y)\n",
    "                val_loss += loss.item() * graph_data.num_graphs\n",
    "                probs = out.cpu().numpy().tolist()\n",
    "                preds = (out > 0.5).int().cpu().numpy().tolist()\n",
    "                labels = graph_data.y.int().cpu().numpy().tolist()\n",
    "                val_probs += probs\n",
    "                val_preds += preds\n",
    "                val_labels += labels\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds)\n",
    "\n",
    "        history['val_loss'].append(val_loss / len(val_loader.dataset))\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_preds'] = val_preds\n",
    "        history['val_probs'] = val_probs\n",
    "        history['val_labels'] = val_labels\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}: Train Loss={history['train_loss'][-1]:.4f}, Acc={train_acc:.4f} | Val Loss={history['val_loss'][-1]:.4f}, Acc={val_acc:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_f1)\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_fusion_model.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"\\nâ›” Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facae239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Plotting --------------------\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.legend(); plt.title('Loss'); plt.xlabel('Epoch')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.legend(); plt.title('Accuracy'); plt.xlabel('Epoch')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Legit', 'Phish'], yticklabels=['Legit', 'Phish'])\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\"); plt.title(\"Confusion Matrix\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_probs):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC Curve\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# -------------------- Device --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299bfd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    df = pd.read_json(r\"E:\\Phising_detection\\dataset\\urls\\urls.json\")\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "    char_vocab = build_char_vocab(train_df)\n",
    "    graph_vocab = build_graph_vocab(train_df)\n",
    "\n",
    "    train_data = FusionDataset(train_df, char_vocab, graph_vocab)\n",
    "    val_data = FusionDataset(val_df, char_vocab, graph_vocab)\n",
    "\n",
    "    train_loader = [(char, graph) for char, graph in train_data]\n",
    "    val_loader = [(char, graph) for char, graph in val_data]\n",
    "\n",
    "    from torch_geometric.loader import DataLoader as GeoLoader\n",
    "    train_loader = GeoLoader(train_loader, batch_size=64, shuffle=True)\n",
    "    val_loader = GeoLoader(val_loader, batch_size=64)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FusionModel(cnn_vocab_size=len(char_vocab), gnn_vocab_size=len(graph_vocab)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    model, history = train_fusion(model, train_loader, val_loader, optimizer, criterion, scheduler)\n",
    "\n",
    "    plot_metrics(history)\n",
    "    plot_confusion_matrix(history['val_labels'], history['val_preds'])\n",
    "    plot_roc_curve(history['val_labels'], history['val_probs'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Phising_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
